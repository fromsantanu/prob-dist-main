
# 📝 **Tutorial: Joint Probability Distributions**

---

## ⭐ **1. What are Joint Distributions?**

✅ **Definition (Simple Words):**
Joint distribution shows the **probability of two (or more) random variables occurring together**.

---

### 🔹 **Types:**

1. **Joint PMF (Probability Mass Function):** For **discrete random variables**.
2. **Joint PDF (Probability Density Function):** For **continuous random variables**.

---

## 🎯 **2. Joint PMF**

🔢 **Definition:**
If X and Y are discrete random variables,

$$
P(X=x, Y=y)
$$

is the **joint PMF** giving probability of X=x **and** Y=y happening together.

---

### 💡 **Example (Dice)**

* X = number on first dice
* Y = number on second dice

Joint PMF shows probabilities for all combinations like (1,1), (1,2), ..., (6,6).

---

## 🎯 **3. Joint PDF**

🔢 **Definition:**
For continuous random variables, **joint PDF** gives **density** not direct probability.

$$
f_{X,Y}(x,y)
$$

🔹 **To find probability:**

$$
P(a<X<b, c<Y<d) = \int_a^b \int_c^d f_{X,Y}(x,y) \,dy\,dx
$$

---

### 💡 **Example (Conceptual)**

Height and weight of people may have a joint PDF showing how these two continuous variables relate across the population.

---

## ⭐ **4. Marginal Distributions**

✅ **Definition (Simple Words):**
Marginal distribution is the **individual distribution of one variable** obtained by **summing or integrating out the other variable** from the joint distribution.

---

### 🔢 **Formula (Discrete):**

$$
P_X(x) = \sum_y P(X=x, Y=y)
$$

### 🔢 **Formula (Continuous):**

$$
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dy
$$

---

### 🎯 **Example**

If you have joint PMF of two dice, marginal PMF for X is obtained by adding probabilities across all Y.

---

## ⭐ **5. Conditional Distributions (Joint context)**

✅ **Definition (Simple Words):**
Conditional distribution shows **distribution of one variable given the other**.

---

### 🔢 **Formula (Discrete):**

$$
P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P_Y(y)}
$$

### 🔢 **Formula (Continuous):**

$$
f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}
$$

---

### 🎯 **Example**

Probability of getting X=2 on first dice **given** Y=5 on second dice.

---

## ⭐ **6. Covariance and Correlation**

### 🔹 **Covariance**

✅ **Definition (Simple Words):**
Measures **how two variables change together**.

---

🔢 **Formula:**

$$
Cov(X,Y) = E[(X - E[X])(Y - E[Y])]
$$

### 💡 **Interpretation:**

* Cov(X,Y) > 0 → Variables **increase together**.
* Cov(X,Y) < 0 → One increases, other decreases.
* Cov(X,Y) = 0 → No linear relationship.

---

### 🔹 **Correlation**

✅ **Definition (Simple Words):**
**Standardised measure** of linear relationship between variables.

---

🔢 **Formula:**

$$
\rho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$

* Always between **-1 and +1**.

---

### 💡 **Interpretation:**

* +1 → Perfect positive linear relationship
* -1 → Perfect negative linear relationship
* 0 → No linear relationship

---

### 💻 **Python Example (Covariance and Correlation)**

```python
import numpy as np

X = [1,2,3,4,5]
Y = [2,4,6,8,10]

# Covariance
cov = np.cov(X, Y, bias=True)[0][1]
print("Covariance:", cov)

# Correlation
corr = np.corrcoef(X, Y)[0][1]
print("Correlation:", corr)
```

✅ **Output:**

* Covariance: Positive value
* Correlation: **1.0** (perfect linear relationship)

---

## 🔬 **7. Practice Questions**

1. What is the marginal PMF of X if joint PMF is given as P(X=1,Y=1)=0.2, P(X=1,Y=2)=0.3, P(X=2,Y=1)=0.1, P(X=2,Y=2)=0.4?
2. Define Covariance in simple words.
3. If Cov(X,Y)=0, what does it mean?

---

### ✅ **8. Quick Answers**

1. P(X=1)=0.2+0.3=0.5, P(X=2)=0.1+0.4=0.5
2. Measure of how two variables **change together**.
3. No **linear relationship** between X and Y (but they could still be dependent non-linearly).

---

# 🎯 **Summary**

| Concept                      | Meaning                                       |
| ---------------------------- | --------------------------------------------- |
| **Joint PMF/PDF**            | Probability/density of two variables together |
| **Marginal Distribution**    | Individual distribution obtained from joint   |
| **Conditional Distribution** | Distribution of one variable given the other  |
| **Covariance**               | How two variables vary together               |
| **Correlation**              | Standardised measure of linear relationship   |

---
